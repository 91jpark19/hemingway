{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2: Knowledge Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mstudio/miniconda3/envs/py3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/Volumes/T7/mt-hemingway/data/hypothesis/knowledge_transfer.csv', encoding='utf-8-sig')\n",
    "# data=data.drop(['small', 'big', 'OPUS_small', 'OPUS_big', 'kakao_org', 'M2M100_org', 'M2M100_small', 'kakao_small', 'M2M100_big', 'kakao_big', 'google_org', 'google_small', 'google_big'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original', 'MarianMT_DE', 'MarianMT_DE_M2M100_KO',\n",
       "       'MarianMT_DE_MBart_KO', 'M2M100_DE', 'M2M100_DE_MBart_KO'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2-1: MarianMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/source.spm: 100%|██████████| 768k/768k [00:00<00:00, 820kB/s]\n",
      "Downloading (…)olve/main/target.spm: 100%|██████████| 797k/797k [00:00<00:00, 3.91MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.27M/1.27M [00:00<00:00, 6.69MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 16.3kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.33k/1.33k [00:00<00:00, 725kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 298M/298M [02:49<00:00, 1.75MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 293/293 [00:00<00:00, 112kB/s]\n"
     ]
    }
   ],
   "source": [
    "en_de_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "en_de_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "768it [12:10,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for idx, val in tqdm(data.iterrows()):\n",
    "    for_translate=val['original']\n",
    "    try:\n",
    "        encoded_input=en_de_tokenizer(for_translate, return_tensors='pt')\n",
    "        output=en_de_model.generate(**encoded_input)\n",
    "        out_text=en_de_tokenizer.batch_decode(output, skip_specual_tokens=True)\n",
    "        out_text=out_text[0]\n",
    "        out_text=out_text.replace('<pad>', '').replace('</s>', '')\n",
    "        out_text=out_text.strip()\n",
    "        translated.append(out_text)\n",
    "    except TypeError:\n",
    "        translated.append(val)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MarianMT_DE']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)neration_config.json: 100%|██████████| 233/233 [00:00<00:00, 139kB/s]\n"
     ]
    }
   ],
   "source": [
    "de_ko_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "de_ko_tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"de\", tgt_lang=\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/768 [00:00<?, ?it/s]/Users/mstudio/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 200 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 768/768 [1:04:22<00:00,  5.03s/it]\n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for i in tqdm(data['MarianMT_DE']):\n",
    "    try:\n",
    "        encoded_hi=de_ko_tokenizer(i, return_tensors='pt')\n",
    "        generated_tokens = de_ko_model.generate(**encoded_hi, forced_bos_token_id=de_ko_tokenizer.get_lang_id(\"ko\"), max_length=1024)\n",
    "        translated.append(de_ko_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n",
    "    except TypeError:\n",
    "        translated.append(i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MarianMT_DE_M2M100_KO']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ko_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "de_ko_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "768it [4:11:13, 19.63s/it] \n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for idx, val in tqdm(data.iterrows()):\n",
    "    for_translate=val['MarianMT_DE']\n",
    "    try:\n",
    "        de_ko_tokenizer.src_lang = \"de_DE\"\n",
    "        encoded_hi=de_ko_tokenizer(for_translate, return_tensors='pt', max_length=1024, truncation=True)\n",
    "        generated_tokens = de_ko_model.generate(**encoded_hi, forced_bos_token_id=de_ko_tokenizer.lang_code_to_id[\"ko_KR\"], max_length=1024)\n",
    "        translated.append(de_ko_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n",
    "    except TypeError:\n",
    "        translated.append(val)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['그는 홀로 스키프에 낚시를 했습니다. 그는 홀로 스키프에 낚시를 했습니다. 그는 홀로 스키프에 낚시를 했습니다. 그는 홀로 스키프에 낚시를 했습니다. 그는 홀로 스키프에 낚시를 했습니다.',\n",
       " '40일 동안에 한 남자가 그와 함께 했습니다. 하지만 40일 동안에 한 남자의 부모님이 그에게 말했습니다. 그 old 남자는 이제 궁하게 사라오라고요. 그건 최악의 불행입니다. 그리고 그 남자는 그녀의 명령에 따라 다른 배로 갔습니다. 그 배는 그 첫 주에 세 마리의 좋은 물고를 잡았습니다.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MarianMT_DE_MBart_KO']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Volumes/T7/mt-hemingway/data/hypothesis/knowledge_transfer.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2-2: M2M100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_de_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "en_de_tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"en\", tgt_lang=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 768/768 [1:04:06<00:00,  5.01s/it]\n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for i in tqdm(data['original']):\n",
    "    try:\n",
    "        encoded_hi=en_de_tokenizer(i, return_tensors='pt')\n",
    "        generated_tokens = en_de_model.generate(**encoded_hi, forced_bos_token_id=en_de_tokenizer.get_lang_id(\"de\"))\n",
    "        translated.append(en_de_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n",
    "    except TypeError:\n",
    "        translated.append(i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['M2M100_DE']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ko_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "de_ko_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "768it [3:02:49, 14.28s/it] \n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for idx, val in tqdm(data.iterrows()):\n",
    "    for_translate=val['M2M100_DE']\n",
    "    try:\n",
    "        de_ko_tokenizer.src_lang = \"de_DE\"\n",
    "        encoded_hi=de_ko_tokenizer(for_translate, return_tensors='pt', max_length=1024, truncation=True)\n",
    "        generated_tokens = de_ko_model.generate(**encoded_hi, forced_bos_token_id=de_ko_tokenizer.lang_code_to_id[\"ko_KR\"], max_length=1024)\n",
    "        translated.append(de_ko_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n",
    "    except TypeError:\n",
    "        translated.append(val)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['M2M100_DE_MBart_KO']=translated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Volumes/T7/mt-hemingway/data/hypothesis/knowledge_transfer.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2-3: MBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_de_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "en_de_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "768it [1:07:05,  5.24s/it]\n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for idx, val in tqdm(data.iterrows()):\n",
    "    for_translate=val['original']\n",
    "    try:\n",
    "        en_de_tokenizer.src_lang = \"en_XX\"\n",
    "        encoded_hi=en_de_tokenizer(for_translate, return_tensors='pt', max_length=1024, truncation=True)\n",
    "        generated_tokens = en_de_model.generate(**encoded_hi, forced_bos_token_id=en_de_tokenizer.lang_code_to_id[\"de_DE\"], max_length=1024)\n",
    "        translated.append(en_de_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n",
    "    except TypeError:\n",
    "        translated.append(val)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MBart_DE']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ko_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "de_ko_tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"de\", tgt_lang=\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 768/768 [1:06:00<00:00,  5.16s/it]\n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for i in tqdm(data['MBart_DE']):\n",
    "    try:\n",
    "        encoded_hi=de_ko_tokenizer(i, return_tensors='pt')\n",
    "        generated_tokens = de_ko_model.generate(**encoded_hi, forced_bos_token_id=de_ko_tokenizer.get_lang_id(\"ko\"), max_length=1024)\n",
    "        translated.append(de_ko_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n",
    "    except TypeError:\n",
    "        translated.append(i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MBart_DE_M2M100_KO']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Volumes/T7/mt-hemingway/data/hypothesis/knowledge_transfer.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2-4: M2M100 + M2M100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ko_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "de_ko_tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"de\", tgt_lang=\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 768/768 [59:51<00:00,  4.68s/it]  \n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for i in tqdm(data['M2M100_DE']):\n",
    "    try:\n",
    "        encoded_hi=de_ko_tokenizer(i, return_tensors='pt')\n",
    "        generated_tokens = de_ko_model.generate(**encoded_hi, forced_bos_token_id=de_ko_tokenizer.get_lang_id(\"ko\"), max_length=1024)\n",
    "        translated.append(de_ko_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n",
    "    except TypeError:\n",
    "        translated.append(i)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['M2M100_DE_M2M100_KO']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Volumes/T7/mt-hemingway/data/hypothesis/knowledge_transfer.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2-5: MBart + MBart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ko_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "de_ko_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "768it [3:59:46, 18.73s/it] \n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for idx, val in tqdm(data.iterrows()):\n",
    "    for_translate=val['MBart_DE']\n",
    "    try:\n",
    "        de_ko_tokenizer.src_lang = \"de_DE\"\n",
    "        encoded_hi=de_ko_tokenizer(for_translate, return_tensors='pt', max_length=1024, truncation=True)\n",
    "        generated_tokens = de_ko_model.generate(**encoded_hi, forced_bos_token_id=de_ko_tokenizer.lang_code_to_id[\"ko_KR\"], max_length=1024)\n",
    "        translated.append(de_ko_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n",
    "    except TypeError:\n",
    "        translated.append(val)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MBart_DE_MBart_KO']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/Volumes/T7/mt-hemingway/data/hypothesis/knowledge_transfer.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1: Iterative Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-1. MarianMT first round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1=pd.read_csv('/Volumes/T7/mt-hemingway/data/book_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1=exp1.drop(['small', 'big', 'OPUS_small', 'OPUS_big', 'kakao_org', 'M2M100_org', 'M2M100_small', 'kakao_small', 'M2M100_big', 'kakao_big', 'google_org', 'google_small', 'google_big'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ko_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
    "en_ko_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/mstudio/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "768it [52:31,  4.10s/it]\n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for idx, val in tqdm(exp1.iterrows()):\n",
    "    for_translate=val['original']\n",
    "    try:\n",
    "        encoded_input=en_ko_tokenizer(for_translate, return_tensors='pt')\n",
    "        output=en_ko_model.generate(**encoded_input)\n",
    "        out_text=en_ko_tokenizer.batch_decode(output, skip_specual_tokens=True)\n",
    "        out_text=out_text[0]\n",
    "        out_text=out_text.replace('<pad>', '').replace('</s>', '')\n",
    "        out_text=out_text.strip()\n",
    "        translated.append(out_text)\n",
    "    except TypeError:\n",
    "        translated.append(val)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1['MarianMT_ko1']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_en_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-ko-en\")\n",
    "ko_en_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-ko-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/mstudio/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "539it [1:11:07,  7.62s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
      "768it [1:48:36,  8.48s/it]\n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for idx, val in tqdm(exp1.iterrows()):\n",
    "    for_translate=val['MarianMT_ko1']\n",
    "    try:\n",
    "        encoded_input=ko_en_tokenizer(for_translate, return_tensors='pt')\n",
    "        output=ko_en_model.generate(**encoded_input)\n",
    "        out_text=ko_en_tokenizer.batch_decode(output, skip_specual_tokens=True)\n",
    "        out_text=out_text[0]\n",
    "        out_text=out_text.replace('<pad>', '').replace('</s>', '')\n",
    "        out_text=out_text.strip()\n",
    "        translated.append(out_text)\n",
    "    except TypeError:\n",
    "        translated.append(val)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1['MarianMT_EN2']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ko_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
    "en_ko_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/mstudio/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "768it [1:14:46,  5.84s/it]\n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for idx, val in tqdm(exp1.iterrows()):\n",
    "    for_translate=val['MarianMT_EN2']\n",
    "    try:\n",
    "        encoded_input=en_ko_tokenizer(for_translate, return_tensors='pt')\n",
    "        output=en_ko_model.generate(**encoded_input)\n",
    "        out_text=en_ko_tokenizer.batch_decode(output, skip_specual_tokens=True)\n",
    "        out_text=out_text[0]\n",
    "        out_text=out_text.replace('<pad>', '').replace('</s>', '')\n",
    "        out_text=out_text.strip()\n",
    "        translated.append(out_text)\n",
    "    except TypeError:\n",
    "        translated.append(val)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1['MarianMT_ko3']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1.to_csv('/Volumes/T7/mt-hemingway/data/hypothesis/exp1.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1=pd.read_csv('/Volumes/T7/mt-hemingway/data/hypothesis/exp1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_en_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-ko-en\")\n",
    "ko_en_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-ko-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/mstudio/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "83it [08:05,  3.96s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (761 > 512). Running this sequence through the model will result in indexing errors\n",
      "768it [1:23:45,  6.54s/it]\n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for idx, val in tqdm(exp1.iterrows()):\n",
    "    for_translate=val['MarianMT_ko3']\n",
    "    try:\n",
    "        encoded_input=ko_en_tokenizer(for_translate, return_tensors='pt')\n",
    "        output=ko_en_model.generate(**encoded_input)\n",
    "        out_text=ko_en_tokenizer.batch_decode(output, skip_specual_tokens=True)\n",
    "        out_text=out_text[0]\n",
    "        out_text=out_text.replace('<pad>', '').replace('</s>', '')\n",
    "        out_text=out_text.strip()\n",
    "        translated.append(out_text)\n",
    "    except TypeError:\n",
    "        translated.append(val)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1['MarianMT_EN4']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1['MarianMT_ko1']=exp1['MarianMT_ko1'].str.replace('<unk>', '')\n",
    "exp1['MarianMT_EN2']=exp1['MarianMT_EN2'].str.replace('<unk>', '')\n",
    "exp1['MarianMT_ko3']=exp1['MarianMT_ko3'].str.replace('<unk>', '')\n",
    "exp1['MarianMT_EN4']=exp1['MarianMT_EN4'].str.replace('<unk>', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ko_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")\n",
    "en_ko_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/mstudio/miniconda3/envs/py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "768it [11:38,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "translated=[]\n",
    "for idx, val in tqdm(exp1.iterrows()):\n",
    "    for_translate=val['MarianMT_EN4']\n",
    "    try:\n",
    "        encoded_input=en_ko_tokenizer(for_translate, return_tensors='pt')\n",
    "        output=en_ko_model.generate(**encoded_input)\n",
    "        out_text=en_ko_tokenizer.batch_decode(output, skip_specual_tokens=True)\n",
    "        out_text=out_text[0]\n",
    "        out_text=out_text.replace('<pad>', '').replace('</s>', '')\n",
    "        out_text=out_text.strip()\n",
    "        translated.append(out_text)\n",
    "    except TypeError:\n",
    "        translated.append(val)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1['MarianMT_ko5']=translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>MarianMT_ko1</th>\n",
       "      <th>MarianMT_EN2</th>\n",
       "      <th>MarianMT_ko3</th>\n",
       "      <th>MarianMT_EN4</th>\n",
       "      <th>MarianMT_ko5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was an old man who fished alone in a skiff ...</td>\n",
       "      <td>Penther historical Cana Adult portfolio until ...</td>\n",
       "      <td>He was trademarked by the detailed man23 wise ...</td>\n",
       "      <td>기술을 잘 때까지 Penther Adultport lio Sharether Chin...</td>\n",
       "      <td>He was man23 Gal 600 was a  ff a             ...</td>\n",
       "      <td>펜더 어덜트 포트 리오 공급 업체 인 Chinas Xiaomi Chinas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the first forty days a boy had been with hi...</td>\n",
       "      <td>Un wellgrad、 China、866 exhibits wiserator. fli...</td>\n",
       "      <td>There is no In, but first a all the  between J...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>이름 *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It made the boy sad to see the old man come in...</td>\n",
       "      <td>웨이브 동맥  하트 킨 웰 카나 성인 솔직히 히로 비난 지혜 파업  샤오 미 테크 ...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>이름 *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The sail was patched with flour sacks and, fur...</td>\n",
       "      <td>process、ther。 지혜 、 ING 끝까지, (32, CD、 명예는 잘。).</td>\n",
       "      <td>The was    , led, it  -.</td>\n",
       "      <td>process、ther、 、 (프로세스、더、더、시작、CD、).</td>\n",
       "      <td>It was a sleight of hand.</td>\n",
       "      <td>wavether Chinasing은 지속 가능성을 제공합니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The old man was thin and gaunt with deep wrink...</td>\n",
       "      <td>process Cana Adultther endque wisdom ing upon ...</td>\n",
       "      <td>Thet details man was and g author Social  s ha...</td>\n",
       "      <td>과정 에서、 Adultther、 endque、 examination、sing 위 요...</td>\n",
       "      <td>All the efforts were made, all the efforts wer...</td>\n",
       "      <td>1.5 well-HBM 동맥, 도매 well-HBM 동맥, 도매 well-HBM 동...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>What's that? she asked a waiter and pointed to...</td>\n",
       "      <td>VipTube Hoteling values forUAL、 China、 His end...</td>\n",
       "      <td>philosophic km's? 2015  a er and to  but not b...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>이름 *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>Tiburon, the waiter said, \"Eshark.\" He was mea...</td>\n",
       "      <td>다운로드、 중국어, 잘、 그의 모기, 에너지 수집 내장 최고의 Penther、 he...</td>\n",
       "      <td>He was, he was, he was, he was, he was, he was...</td>\n",
       "      <td>펜더, 메이커더, 메이커더, 메이커더, 메이커더, 메이커더, 메이커더</td>\n",
       "      <td>,,,,,,</td>\n",
       "      <td>(,,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>I didn't know sharks had such handsome, beauti...</td>\n",
       "      <td>US 호텔 에서 단독으로 866 tentacle,   .</td>\n",
       "      <td>I am a sinner, a sinner, a sinner, a sinner in...</td>\n",
       "      <td>미국 심리 China, China, China, China upon Ford.(포드...</td>\n",
       "      <td>.</td>\n",
       "      <td>에스.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>I didn't either, her male companion said.</td>\n",
       "      <td>US 호텔 From,리스 모기.</td>\n",
       "      <td>I Hatet Innovate.</td>\n",
       "      <td>US‐From‐Un‐ vehicle‐s에서 제공합니다.</td>\n",
       "      <td>I'm sorry, but I'm only 9th graders in Japan.</td>\n",
       "      <td>US 호텔 8 월, 수용 US 호텔 8Firstve는 OD에 trust-ically.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>Up the road, in his shack, the old man was sle...</td>\n",
       "      <td>책 잘, strikesing 시, 잘 카나 Adultther ND. 펜터는 성공 파...</td>\n",
       "      <td>occurred in Fran 320,  man was               ...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>이름 *</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              original  \\\n",
       "0    He was an old man who fished alone in a skiff ...   \n",
       "1    In the first forty days a boy had been with hi...   \n",
       "2    It made the boy sad to see the old man come in...   \n",
       "3    The sail was patched with flour sacks and, fur...   \n",
       "4    The old man was thin and gaunt with deep wrink...   \n",
       "..                                                 ...   \n",
       "763  What's that? she asked a waiter and pointed to...   \n",
       "764  Tiburon, the waiter said, \"Eshark.\" He was mea...   \n",
       "765  I didn't know sharks had such handsome, beauti...   \n",
       "766          I didn't either, her male companion said.   \n",
       "767  Up the road, in his shack, the old man was sle...   \n",
       "\n",
       "                                          MarianMT_ko1  \\\n",
       "0    Penther historical Cana Adult portfolio until ...   \n",
       "1    Un wellgrad、 China、866 exhibits wiserator. fli...   \n",
       "2    웨이브 동맥  하트 킨 웰 카나 성인 솔직히 히로 비난 지혜 파업  샤오 미 테크 ...   \n",
       "3        process、ther。 지혜 、 ING 끝까지, (32, CD、 명예는 잘。).   \n",
       "4    process Cana Adultther endque wisdom ing upon ...   \n",
       "..                                                 ...   \n",
       "763  VipTube Hoteling values forUAL、 China、 His end...   \n",
       "764  다운로드、 중국어, 잘、 그의 모기, 에너지 수집 내장 최고의 Penther、 he...   \n",
       "765                    US 호텔 에서 단독으로 866 tentacle,   .   \n",
       "766                                  US 호텔 From,리스 모기.   \n",
       "767  책 잘, strikesing 시, 잘 카나 Adultther ND. 펜터는 성공 파...   \n",
       "\n",
       "                                          MarianMT_EN2  \\\n",
       "0    He was trademarked by the detailed man23 wise ...   \n",
       "1    There is no In, but first a all the  between J...   \n",
       "2                                                  ...   \n",
       "3                             The was    , led, it  -.   \n",
       "4    Thet details man was and g author Social  s ha...   \n",
       "..                                                 ...   \n",
       "763  philosophic km's? 2015  a er and to  but not b...   \n",
       "764  He was, he was, he was, he was, he was, he was...   \n",
       "765  I am a sinner, a sinner, a sinner, a sinner in...   \n",
       "766                                  I Hatet Innovate.   \n",
       "767   occurred in Fran 320,  man was               ...   \n",
       "\n",
       "                                          MarianMT_ko3  \\\n",
       "0    기술을 잘 때까지 Penther Adultport lio Sharether Chin...   \n",
       "1                                                  ...   \n",
       "2                                                        \n",
       "3                   process、ther、 、 (프로세스、더、더、시작、CD、).   \n",
       "4    과정 에서、 Adultther、 endque、 examination、sing 위 요...   \n",
       "..                                                 ...   \n",
       "763                                                ...   \n",
       "764             펜더, 메이커더, 메이커더, 메이커더, 메이커더, 메이커더, 메이커더   \n",
       "765  미국 심리 China, China, China, China upon Ford.(포드...   \n",
       "766                     US‐From‐Un‐ vehicle‐s에서 제공합니다.   \n",
       "767                                                ...   \n",
       "\n",
       "                                          MarianMT_EN4  \\\n",
       "0     He was man23 Gal 600 was a  ff a             ...   \n",
       "1                                                  ...   \n",
       "2                                                        \n",
       "3                            It was a sleight of hand.   \n",
       "4    All the efforts were made, all the efforts wer...   \n",
       "..                                                 ...   \n",
       "763                                                      \n",
       "764                                             ,,,,,,   \n",
       "765                                                  .   \n",
       "766      I'm sorry, but I'm only 9th graders in Japan.   \n",
       "767                                                      \n",
       "\n",
       "                                          MarianMT_ko5  \n",
       "0           펜더 어덜트 포트 리오 공급 업체 인 Chinas Xiaomi Chinas.  \n",
       "1                                                 이름 *  \n",
       "2                                                 이름 *  \n",
       "3                   wavether Chinasing은 지속 가능성을 제공합니다.  \n",
       "4    1.5 well-HBM 동맥, 도매 well-HBM 동맥, 도매 well-HBM 동...  \n",
       "..                                                 ...  \n",
       "763                                               이름 *  \n",
       "764                                              (,,,,  \n",
       "765                                                에스.  \n",
       "766    US 호텔 8 월, 수용 US 호텔 8Firstve는 OD에 trust-ically.  \n",
       "767                                               이름 *  \n",
       "\n",
       "[768 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1.to_csv('/Volumes/T7/mt-hemingway/data/hypothesis/exp1.csv', encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c8f0e5dedb0bca84d2e0b4fbfa2d31163667101b0d2986ebd077634811ae380"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
